{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from konlpy.tag import Kkma\n",
    "import pandas as pd\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "\n",
    "kkma = Kkma()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "겟아웃_연관성.csv 저장 완료\n",
      "부산행_연관성.csv 저장 완료\n",
      "서치_연관성.csv 저장 완료\n",
      "해피데스데이_연관성.csv 저장 완료\n",
      "곡성_연관성.csv 저장 완료\n"
     ]
    }
   ],
   "source": [
    "movie_title={1:\"겟아웃\",2:\"부산행\",3:\"서치\",4:\"해피데스데이\",5:\"곡성\"}\n",
    "for i in range(len(movie_title)):\n",
    "    rtitle = \"%s_최종본.csv\" %(movie_title[i+1])\n",
    "    uptitle= \"%s_상위tf.csv\" %(movie_title[i+1])\n",
    "    # Get out, Train to Busan, Search over 3.58, The Wailing over 3.81, Happy Deathday over 3.32이상\n",
    "    wtitle = \"%s_연관성.csv\" %(movie_title[i+1])\n",
    "    \n",
    "    csv_file = open(rtitle, 'r')\n",
    "    doc = csv.reader(csv_file)\n",
    "    next(doc) # skip header\n",
    "    \n",
    "    # Split by sentence\n",
    "    def split_sentence(doc):\n",
    "        text = doc[2]\n",
    "        sentence=kkma.sentences(text)\n",
    "        return sentence\n",
    "    # Save by sentence\n",
    "    result = []\n",
    "    for line in doc:\n",
    "        result.extend(split_sentence(line))\n",
    "    \n",
    "    ## Top tf Preprocess2\n",
    "    f=open(uptitle,'r')\n",
    "    rf=csv.reader(f)\n",
    "    next(rf)\n",
    "    TF=[]\n",
    "    for row in rf:\n",
    "        TF.append(row[0])\n",
    "\n",
    "    stop=['이', '을', '스', '여','노', '은', '아', '터', '드', '마', '자', '고', '레','거','개방','관람','씬','내용','결국','그런데',\n",
    "          '거','늘', '의', '라', '누', '어', '건','진','자', '마','로','스','까','은','시','드','곽','왜','그리하','약간','갑자기',\n",
    "          '마','저','거','라','까','분','늘','자','의','어','스','여','을','몇','몇몇','걸','시','드','단','서','인','블록버스터',\n",
    "          '늘','메','파','자','김','티','리','바','스','건','대','새','어','보','시','디','르','을','고','예고편','스포일러','장르',\n",
    "          '어쩌','듯하','경','어느','결말','이때','재미있','그리하','이번','조금',\n",
    "          '스','자','누','터','라','뜨','다','막','분','임','위','하다','리','사','유','면','감독','스포','줄거리','영화']\n",
    "    newTF=[]\n",
    "    for word in TF:\n",
    "        if word not in stop:\n",
    "            newTF.append(word)\n",
    "\n",
    "    # Analyze results divided by sentence by morpheme\n",
    "    def split_morphs(doc):\n",
    "        doc_morphs = kkma.morphs(doc)\n",
    "        return doc_morphs\n",
    "    \n",
    "    dataset2=[]\n",
    "    for i in range(len(result)):\n",
    "        a=split_morphs(result[i])\n",
    "        dataset= []\n",
    "        for w in a:\n",
    "            if w in newTF: \n",
    "                dataset.append(w)\n",
    "        dataset2.append(dataset)\n",
    "    \n",
    "    # Check which top tf word each sentence has\n",
    "    dataset3=[]\n",
    "    for k in range(len(dataset2)):\n",
    "        if dataset2[k] != []:\n",
    "            dataset3.append(dataset2[k])\n",
    "    \n",
    "    # Association Analyze \n",
    "    te = TransactionEncoder()\n",
    "    te_ary = te.fit(dataset3).transform(dataset3)\n",
    "    df = pd.DataFrame(te_ary, columns=te.columns_) #for better understanding chage into Dataframe \n",
    "    \n",
    "    frequent_itemsets3 = apriori(df, min_support=0.003, max_len=3, use_colnames=True)\n",
    "    frequent_itemsets2 = apriori(df, min_support=0.003, max_len=2, use_colnames=True)\n",
    "    frequentThree=frequent_itemsets3[len(frequent_itemsets2):len(frequent_itemsets3)]    \n",
    "    \n",
    "    frequentThree.to_csv(wtitle, encoding = 'cp949')       \n",
    "    print(wtitle,\"저장 완료\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
